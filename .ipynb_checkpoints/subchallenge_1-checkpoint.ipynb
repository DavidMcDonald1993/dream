{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from fancyimpute import SimpleFill, KNN, SoftImpute, IterativeSVD, MICE, MatrixFactorization, NuclearNormMinimization\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.objectives import binary_crossentropy, mean_squared_error\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../data/sub_challenge_1/data_obs_1.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a97a55cc4781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a97a55cc4781>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     dfs = [pd.read_csv(\"../data/sub_challenge_1/data_obs_{}.txt\".format(i), \n\u001b[0;32m--> 162\u001b[0;31m                     header=0, index_col=0, sep=\"\\t\") for i in range(1, 11)]\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m# ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../data/sub_challenge_1/data_obs_1.txt does not exist"
     ]
    }
   ],
   "source": [
    "# root mean squared error with three masks\n",
    "def rmse (original_data, y_pred, y_true): \n",
    "    # rsme prediction and ground truth\n",
    "    rmse_no_mask = np.sqrt(mse(y_true, y_pred))\n",
    "    \n",
    "    # ignore all zeros in the ground truth data\n",
    "    no_zeros = y_true > 0\n",
    "    rmse_no_zeros = np.sqrt(mse(y_true[no_zeros], y_pred[no_zeros]))\n",
    "    \n",
    "    # ignore zeros and only consider data that was originally nan in the training data\n",
    "    nan_no_zeros = np.isnan(original_data) & (y_true > 0)\n",
    "    rmse_nan_no_zeros = np.sqrt(mse(y_true[nan_no_zeros], y_pred[nan_no_zeros]))\n",
    "    \n",
    "    # concatenate all three results\n",
    "    return np.array([rmse_no_mask, rmse_no_zeros, rmse_nan_no_zeros])\n",
    "\n",
    "# compute mean rmse across a number of repreats\n",
    "def mean_rmse(data, imputation_method, num_repeats=1, **kwargs):\n",
    "    \n",
    "    imputed_predictions = [imputation_method(data, **kwargs) for i in range(num_repeats)]\n",
    "    \n",
    "    rmses = np.array([rmse(data, imputed_prediction) for imputed_prediction in imputed_predictions])\n",
    "\n",
    "    return rmses.mean(axis=0)\n",
    "\n",
    "\n",
    "# imputation methods\n",
    "# impute with sample mean\n",
    "def sample_mean(data, **kwargs):\n",
    "    fill = SimpleFill(fill_method=\"mean\")\n",
    "    return fill.complete(data)\n",
    "\n",
    "# impute with knn-3\n",
    "def knn_3(data, **kwargs):\n",
    "    fill = KNN(k=3, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# impute with knn-5\n",
    "def knn_5(data, **kwargs):\n",
    "    fill = KNN(k=5, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# knn for any k\n",
    "def knn(data, k, **kwargs):\n",
    "    fill = KNN(k=k, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# softimpute from fancyimpute package\n",
    "def soft_impute(data, **kwargs):\n",
    "    fill = SoftImpute(verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# iterativeSVD from fancy impute package\n",
    "def iterative_SVD(data, **kwargs):\n",
    "    fill = IterativeSVD(verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# MICE for fancyimpute package\n",
    "def mice(data, **kwargs):\n",
    "    fill = MICE(verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# modified autoencoder that does not propagate error from missing values\n",
    "def modified_autoencoder(data, num_hidden=[32], dropout=0.1, **kwargs):\n",
    "    \n",
    "    # to normalise the data we must impute \n",
    "    mean_imputer = SimpleFill(fill_method=\"mean\")\n",
    "    data_imputed = mean_imputer.complete(data)\n",
    "    \n",
    "    # standard scaling for normalisation\n",
    "    standard_scaler = StandardScaler()\n",
    "    data_imputed_and_scaled = standard_scaler.fit_transform(data_imputed)\n",
    "    \n",
    "    # replace all missing values with 0 so they do not contribute to input\n",
    "    data_imputed_and_scaled[np.isnan(data)] = 0\n",
    "    \n",
    "    # maintain nan in target data so we know which outputs should not prodice any error\n",
    "    data_scaled_with_nan = np.array([[data_imputed_and_scaled[i, j] if ~np.isnan(data[i, j]) else np.nan\n",
    "                                     for j in range(num_features)] for i in range(num_samples)])\n",
    "    \n",
    "    # custom MSE that only produces error on non-nan terms\n",
    "    def custom_MSE(y_true, y_pred):\n",
    "    \n",
    "        y_true = K.flatten(y_true)\n",
    "        y_pred = K.flatten(y_pred)\n",
    "\n",
    "        # mask for targets that are not nan\n",
    "        mask = ~tf.is_nan(y_true)\n",
    "\n",
    "        # apply the mask to targets and output of network and then compute MSE with what remains\n",
    "        y_true = tf.boolean_mask(tensor=y_true, mask=mask)\n",
    "        y_pred = tf.boolean_mask(tensor=y_pred, mask=mask)\n",
    "\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    # construct model\n",
    "    x = Input(shape=(num_features,))\n",
    "    \n",
    "    # first fully connected layer layer\n",
    "    y = Dense(num_hidden[0], activation=\"relu\")(x)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(dropout)(y)\n",
    "\n",
    "    # all remaining fully connected layers\n",
    "    for h in num_hidden[1:] + num_hidden[-2::-1]:\n",
    "        y = Dense(h, activation=\"relu\")(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(dropout)(y)\n",
    "    \n",
    "    # output -- no activation function \n",
    "    y = Dense(num_features, activation=\"linear\")(y)\n",
    "    autoencoder = Model(x, y)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=custom_binary_crossentropy)\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience=1000, min_delta=0)\n",
    "    tqdm = TQDMNotebookCallback(leave_inner=False, leave_outer=True)\n",
    "    # train model\n",
    "    autoencoder.fit(data_imputed_and_scaled, data_scaled_with_nan, \n",
    "                    verbose=0, epochs=10000, batch_size=100, callbacks=[early_stopping, tqdm])\n",
    "    # predict data\n",
    "    prediction = autoencoder.predict(data_imputed_and_scaled)\n",
    "    \n",
    "    # reverse normalise and return\n",
    "    return standard_scaler.inverse_transform(prediction)\n",
    "\n",
    "# PCA and then autoencoder\n",
    "def pca_autoencoder(data, num_hidden=[32], dropout=0.1, pca_dim=64, **kwargs):\n",
    "    \n",
    "    #construct model\n",
    "    x = Input(shape=(pca_dim,))\n",
    "    y = Dropout(1e-8)(x)\n",
    "    for h in num_hidden + num_hidden[-2::-1]:\n",
    "        y = Dense(h, activation=\"relu\")(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(dropout)(y)\n",
    "    y = Dense(pca_dim)(y)\n",
    "    \n",
    "    autoencoder = Model(x, y)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    \n",
    "    \n",
    "    # project with pca\n",
    "    mean_imputer = SimpleFill()\n",
    "    data_imputed = mean_imputer.complete(data)\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    data_transformed = pca.fit_transform(data_imputed)\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience=100, min_delta=0)\n",
    "    tqdm = TQDMNotebookCallback(leave_inner=False, leave_outer=True)\n",
    "    autoencoder.fit(data_transformed, data_transformed, \n",
    "                    verbose=0, epochs=10000, batch_size=100, callbacks=[early_stopping, tqdm])\n",
    "    \n",
    "    prediction = autoencoder.predict(data_transformed)\n",
    "    \n",
    "    return pca.inverse_transform(prediction)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # training data\n",
    "    dfs = [pd.read_csv(\"../data/sub_challenge_1/data_obs_{}.txt\".format(i), \n",
    "                    header=0, index_col=0, sep=\"\\t\") for i in range(1, 11)]\n",
    "\n",
    "    # ground truth\n",
    "    ground_truth_table = pd.read_csv(\"../data/sub_challenge_1/data_true.txt\", \n",
    "                    header=0, index_col=0, sep=\"\\t\")\n",
    "\n",
    "    # conver from data frame ot numpy array\n",
    "    datas = [df.values for df in dfs]\n",
    "    ground_truth = ground_truth_table.values\n",
    "\n",
    "    # dimensionality of data\n",
    "    num_samples, num_features = ground_truth.shape\n",
    "\n",
    "    # list of imputation tecniques\n",
    "#     imputation_methods = [sample_mean, knn_3, knn_5, soft_impute, iterative_SVD, mice,\n",
    "#                           modified_autoencoder, pca_autoencoder]\n",
    "    imputation_method = [sample_mean]\n",
    "    \n",
    "    # iterate over all training data and imputation methods and compute mean rmse for num repeats\n",
    "    rmses = np.array([[mean_rmse(data, imputation_method, num_repeats=1) for data in datas] \n",
    "                      for imputation_method in imputation_methods])\n",
    "    \n",
    "    print rmses.shape\n",
    "    \n",
    "    # save to file\n",
    "#     np.savetxt(X=rsmes)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
