{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Computing rmse\n",
      "Saving rmse to file\n",
      "RMSE\n",
      "[[ 0.48025014  0.48158118  0.48047411  0.47901986  0.48033528  0.48376452\n",
      "   0.48452314  0.48262392  0.48283767  0.48443891]\n",
      " [ 0.3837956   0.37947918  0.38237012  0.38034215  0.38368019  0.38396877\n",
      "   0.38545407  0.38270399  0.38047192  0.38018053]\n",
      " [ 0.48294988  0.47640953  0.48014478  0.47834866  0.48017847  0.48542613\n",
      "   0.48424895  0.48213823  0.48037038  0.4830848 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from fancyimpute import SimpleFill, KNN, SoftImpute, IterativeSVD, MICE, MatrixFactorization, NuclearNormMinimization\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from keras.objectives import binary_crossentropy, mean_squared_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "# root mean squared error with three masks\n",
    "def rmse (original_data, y_pred, y_true): \n",
    "    # rsme prediction and ground truth\n",
    "    rmse_no_mask = np.sqrt(mse(y_true, y_pred))\n",
    "    \n",
    "    # ignore all zeros in the ground truth data\n",
    "    no_zeros = y_true > 0\n",
    "    rmse_no_zeros = np.sqrt(mse(y_true[no_zeros], y_pred[no_zeros]))\n",
    "    \n",
    "    # ignore zeros and only consider data that was originally nan in the training data\n",
    "    nan_no_zeros = np.isnan(original_data) & (y_true > 0)\n",
    "    rmse_nan_no_zeros = np.sqrt(mse(y_true[nan_no_zeros], y_pred[nan_no_zeros]))\n",
    "    \n",
    "    # concatenate all three results\n",
    "    return np.array([rmse_no_mask, rmse_no_zeros, rmse_nan_no_zeros])\n",
    "\n",
    "# compute mean rmse across a number of repreats\n",
    "def mean_rmse(data, imputation_method, y_true, num_repeats=1, **kwargs):\n",
    "    \n",
    "    imputed_predictions = [imputation_method(data, **kwargs) for i in range(num_repeats)]\n",
    "    \n",
    "    rmses = np.array([rmse(data, imputed_prediction, y_true) for imputed_prediction in imputed_predictions])\n",
    "\n",
    "    return rmses.mean(axis=0)\n",
    "\n",
    "# imputation methods\n",
    "# impute with sample mean\n",
    "def sample_mean(data, **kwargs):\n",
    "    fill = SimpleFill(fill_method=\"mean\")\n",
    "    return fill.complete(data)\n",
    "\n",
    "# impute with knn-3\n",
    "def knn_3(data, **kwargs):\n",
    "    fill = KNN(k=3, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# impute with knn-5\n",
    "def knn_5(data, **kwargs):\n",
    "    fill = KNN(k=5, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# knn for any k\n",
    "def knn(data, k, **kwargs):\n",
    "    fill = KNN(k=k, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "def knn_bootstrap(data, k, sample_size, num_samples, **kwargs):\n",
    "    \n",
    "    num_proteins = data.shape[0]\n",
    "    \n",
    "    idxs = np.array([])\n",
    "    imputed_predictions = None\n",
    "    \n",
    "    sample = 0\n",
    "    while sample < num_samples and np.array([x not in idxs for x in range(num_proteins)]).any():\n",
    "        \n",
    "        idx = np.random.choice(num_proteins, sample_size, replace=True)\n",
    "        \n",
    "        bootstrap_sample = data[idx]\n",
    "        imputed_prediction = knn(bootstrap_sample, k, **kwargs)\n",
    "        \n",
    "        idxs = np.append(idxs, idx)\n",
    "        if imputed_predictions is None:\n",
    "            imputed_predictions = imputed_prediction\n",
    "        else:\n",
    "            imputed_predictions = np.row_stack([imputed_predictions, imputed_prediction])\n",
    "                                      \n",
    "    imputed_predictions = np.array(imputed_predictions)\n",
    "    \n",
    "    complete_prediction = np.zeros_like(data)\n",
    "    \n",
    "    for i in range(num_proteins):\n",
    "        \n",
    "        row_predictions = imputed_predictions[np.where(idxs==i)[0]]\n",
    "        if len(row_predictions) == 1:\n",
    "            complete_prediction[i] = row_predictions\n",
    "        else:\n",
    "            complete_prediction[i] = row_predictions.mean(axis=0)\n",
    "        \n",
    "    return complete_prediction\n",
    "\n",
    "\n",
    "\n",
    "# softimpute from fancyimpute package\n",
    "# def soft_impute(data, **kwargs):\n",
    "#     fill = SoftImpute(verbose=0)\n",
    "#     return fill.complete(data)\n",
    "\n",
    "# removing to focus on optimising iterativeSVD\n",
    "\n",
    "# # iterativeSVD from fancy impute package\n",
    "def iterative_SVD(data, **kwargs):\n",
    "    fill = IterativeSVD(verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# # MICE for fancyimpute package\n",
    "# def mice(data, **kwargs):\n",
    "#     fill = MICE(verbose=0)\n",
    "#     return fill.complete(data)\n",
    "\n",
    "# modified autoencoder that does not propagate error from missing values\n",
    "def modified_autoencoder(data, num_hidden=[32], dropout=0.1, **kwargs):\n",
    "    \n",
    "    # dimensionality of data\n",
    "    num_proteins, num_features = data.shape\n",
    "    \n",
    "    # to normalise the data we must impute \n",
    "    mean_imputer = SimpleFill(fill_method=\"mean\")\n",
    "    data_imputed = mean_imputer.complete(data)\n",
    "    \n",
    "    # standard scaling for normalisation\n",
    "    standard_scaler = StandardScaler()\n",
    "    data_imputed_and_scaled = standard_scaler.fit_transform(data_imputed)\n",
    "    \n",
    "    # replace all missing values with 0 so they do not contribute to input\n",
    "    data_imputed_and_scaled[np.isnan(data)] = 0\n",
    "    \n",
    "    # maintain nan in target data so we know which outputs should not prodice any error\n",
    "    data_scaled_with_nan = np.array([[data_imputed_and_scaled[i, j] if ~np.isnan(data[i, j]) else np.nan\n",
    "                                     for j in range(num_features)] for i in range(num_proteins)])\n",
    "    \n",
    "    # custom MSE that only produces error on non-nan terms\n",
    "    def custom_MSE(y_true, y_pred):\n",
    "    \n",
    "        y_true = K.flatten(y_true)\n",
    "        y_pred = K.flatten(y_pred)\n",
    "\n",
    "        # mask for targets that are not nan\n",
    "        mask = ~tf.is_nan(y_true)\n",
    "\n",
    "        # apply the mask to targets and output of network and then compute MSE with what remains\n",
    "        y_true = tf.boolean_mask(tensor=y_true, mask=mask)\n",
    "        y_pred = tf.boolean_mask(tensor=y_pred, mask=mask)\n",
    "\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    # construct model\n",
    "    x = Input(shape=(num_features,))\n",
    "    \n",
    "    # first fully connected layer layer\n",
    "    y = Dense(num_hidden[0], activation=\"relu\")(x)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(dropout)(y)\n",
    "\n",
    "    # all remaining fully connected layers\n",
    "    for h in num_hidden[1:] + num_hidden[-2::-1]:\n",
    "        y = Dense(h, activation=\"relu\")(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(dropout)(y)\n",
    "    \n",
    "    # output -- no activation function \n",
    "    y = Dense(num_features, activation=\"linear\")(y)\n",
    "    autoencoder = Model(x, y)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=custom_MSE)\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience=100, min_delta=0)\n",
    "    # train model\n",
    "    autoencoder.fit(data_imputed_and_scaled, data_scaled_with_nan, \n",
    "                    verbose=0, epochs=10000, callbacks=[early_stopping])\n",
    "    \n",
    "    print \"trained autoencoder\"\n",
    "    # predict data\n",
    "    prediction = autoencoder.predict(data_imputed_and_scaled)\n",
    "    \n",
    "    # reverse normalise and return\n",
    "    return standard_scaler.inverse_transform(prediction)\n",
    "\n",
    "# PCA and then autoencoder\n",
    "def pca_autoencoder(data, num_hidden=[32], dropout=0.1, pca_dim=64, **kwargs):\n",
    "    \n",
    "    \n",
    "    # dimensionality of data\n",
    "    num_proteins, num_features = data.shape\n",
    "    \n",
    "    #construct model\n",
    "    x = Input(shape=(pca_dim,))\n",
    "    y = Dropout(1e-8)(x)\n",
    "    for h in num_hidden + num_hidden[-2::-1]:\n",
    "        y = Dense(h, activation=\"relu\")(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(dropout)(y)\n",
    "    y = Dense(pca_dim)(y)\n",
    "    \n",
    "    autoencoder = Model(x, y)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    \n",
    "    \n",
    "    # project with pca\n",
    "    mean_imputer = SimpleFill()\n",
    "    data_imputed = mean_imputer.complete(data)\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    data_transformed = pca.fit_transform(data_imputed)\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience=1000, min_delta=0)\n",
    "    autoencoder.fit(data_transformed, data_transformed, \n",
    "                    verbose=0, epochs=10000, callbacks=[early_stopping])\n",
    "    \n",
    "    prediction = autoencoder.predict(data_transformed)\n",
    "    \n",
    "    return pca.inverse_transform(prediction)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print \"Loading data\"\n",
    "\n",
    "    # training data\n",
    "    dfs = [pd.read_csv(\"../data/sub_challenge_1/data_obs_{}.txt\".format(i), \n",
    "                    header=0, index_col=0, sep=\"\\t\") for i in range(1, 11)]\n",
    "\n",
    "    # ground truth\n",
    "    ground_truth_table = pd.read_csv(\"../data/sub_challenge_1/data_true.txt\", \n",
    "                    header=0, index_col=0, sep=\"\\t\")\n",
    "\n",
    "    # conver from data frame ot numpy array\n",
    "    datas = [df.values for df in dfs]\n",
    "    ground_truth = ground_truth_table.values\n",
    "\n",
    "    # list of imputation tecniques\n",
    "#     imputation_methods = [sample_mean, knn_3, knn_5, soft_impute, \n",
    "#                           modified_autoencoder, pca_autoencoder]\n",
    "\n",
    "\n",
    "#     imputation_methods = [partial(modified_autoencoder, num_hidden=[32]),#]\n",
    "#                          partial(modified_autoencoder, num_hidden=[64, 32]),\n",
    "#                          partial(modified_autoencoder, num_hidden=[128, 64, 32])]\n",
    "#     imputation_method_names = [\"autoencoder_32\", #]\n",
    "#                                \"autoencoder_64_32\", \n",
    "#                                \"autoencoder_128_64_32\"]\n",
    "#     imputation_methods = [iterative_SVD,\n",
    "#                          partial(iterative_SVD, )]\n",
    "    imputation_methods = [#knn_5, \n",
    "                          partial(knn_bootstrap, sample_size=100, num_samples = 1000, k=5),\n",
    "        partial(knn_bootstrap, sample_size=ground_truth.shape[0], num_samples = 100, k=5),\n",
    "        partial(knn_bootstrap, sample_size=100, num_samples = 1000, k=5),]\n",
    "    imputation_method_names = [\"bootstrap_knn_100_100\", \"bootstrap_knn_num_proteins_100\", \"bootstrap_knn_100_1000\"]\n",
    "    \n",
    "    print \"Computing rmse\"\n",
    "    \n",
    "    # iterate over all training data and imputation methods and compute mean rmse for num repeats\n",
    "    rmses = np.array([[mean_rmse(data, imputation_method, ground_truth, num_repeats=1) for data in datas] \n",
    "                      for imputation_method in imputation_methods])\n",
    "    \n",
    "    print \"Saving rmse to file\"\n",
    "    \n",
    "    data = rmses[:,:,2]\n",
    "    \n",
    "    data_df = pd.DataFrame(data, index=imputation_method_names, \n",
    "                           columns=[\"training_data_{}\".format(i) for i in range(1, 11)])\n",
    "    data_df.to_csv(\"../results/subchallenge_1/{}_rmses_only_nan_ignore_zeros.csv\".format(\"_\".join(imputation_method_names)), sep=\",\")\n",
    "    \n",
    "    \n",
    "    print \"RMSE\"\n",
    "    print data\n",
    "    \n",
    "    # save to file\n",
    "#     np.savetxt(X=rmses[:,:,0], \n",
    "#                fname=\"../results/subchallenge_1/{}_rmses_no_mask.csv\".format(\"_\".join(imputation_method_names)), delimiter=\",\")\n",
    "#     np.savetxt(X=rmses[:,:,1], \n",
    "#                fname=\"../results/subchallenge_1/{}_rmses_ignore_zeros.csv\".format(\"_\".join(imputation_method_names)), delimiter=\",\")\n",
    "#     np.savetxt(X=rmses[:,:,2], \n",
    "#                fname=\"../results/subchallenge_1/{}_rmses_only_nan_ignore_zeros.csv\".format(\"_\".join(imputation_method_names)), \n",
    "#                delimiter=\",\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \n",
    "    df = pd.read_csv(filename, sep=\"\\t\", header=0, index_col=0, )\n",
    "    \n",
    "    return df.values, df.index, df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_bootstrap(data, k, sample_size, num_samples, **kwargs):\n",
    "    \n",
    "    num_proteins = data.shape[0]\n",
    "    \n",
    "    idxs = np.array([])\n",
    "    imputed_predictions = None\n",
    "    \n",
    "    sample = 0\n",
    "    while sample < num_samples and np.array([x not in idxs for x in range(num_proteins)]).any():\n",
    "        \n",
    "        idx = np.random.choice(num_proteins, sample_size, replace=True)\n",
    "        \n",
    "        bootstrap_sample = data[idx]\n",
    "        imputed_prediction = knn(bootstrap_sample, k, **kwargs)\n",
    "        \n",
    "        idxs = np.append(idxs, idx)\n",
    "        if imputed_predictions is None:\n",
    "            imputed_predictions = imputed_prediction\n",
    "        else:\n",
    "            imputed_predictions = np.row_stack([imputed_predictions, imputed_prediction])\n",
    "                                      \n",
    "    imputed_predictions = np.array(imputed_predictions)\n",
    "    \n",
    "    complete_prediction = np.zeros_like(data)\n",
    "    \n",
    "    for i in range(num_proteins):\n",
    "        \n",
    "        row_predictions = imputed_predictions[np.where(idxs==i)[0]]\n",
    "        if len(row_predictions) == 1:\n",
    "            complete_prediction[i] = row_predictions\n",
    "        else:\n",
    "            complete_prediction[i] = row_predictions.mean(axis=0)\n",
    "        \n",
    "    return complete_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, index, columns = load_data(\"../data/sub_challenge_1/data_obs_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30.39102228,  29.10387767,  30.07013839, ...,  29.38377411,\n",
       "         29.53601563,  29.67869975],\n",
       "       [ 30.38635995,  29.09353396,  30.05899567, ...,  29.37105018,\n",
       "         29.52330719,  29.66496016],\n",
       "       [ 29.86446814,  29.09184396,  30.05933845, ...,  29.36936577,\n",
       "         29.52366181,  29.72324976],\n",
       "       ..., \n",
       "       [ 23.08616549,  21.93517499,  23.02122151, ...,  22.40058085,\n",
       "         23.30060219,  23.64940526],\n",
       "       [ 21.26722578,  21.76599461,  21.25573788, ...,  21.32673303,\n",
       "         20.59719677,  21.47215192],\n",
       "       [ 26.3676424 ,  24.64069933,  25.9333985 , ...,  25.5465123 ,\n",
       "         26.21346231,  26.59695673]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_bootstrap(data, sample_size=500, k=5, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4], [3,4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = np.row_stack([a, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  3.,  4.,  5.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
