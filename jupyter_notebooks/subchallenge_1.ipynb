{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Computing rmse\n",
      "trained autoencoder\n",
      "trained autoencoder\n",
      "trained autoencoder\n",
      "trained autoencoder\n",
      "trained autoencoder\n",
      "trained autoencoder\n",
      "trained autoencoder\n",
      "trained autoencoder\n",
      "trained autoencoder\n",
      "trained autoencoder\n",
      "trained autoencoder\n",
      "trained autoencoder\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8a27f5c7bae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-8a27f5c7bae1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# iterate over all training data and imputation methods and compute mean rmse for num repeats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     rmses = np.array([[mean_rmse(data, imputation_method, ground_truth, num_repeats=1) for data in datas] \n\u001b[0;32m--> 213\u001b[0;31m                       for imputation_method in imputation_methods])\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Saving rmse to file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8a27f5c7bae1>\u001b[0m in \u001b[0;36mmean_rmse\u001b[0;34m(data, imputation_method, y_true, num_repeats, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_rmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimputation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mimputed_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimputation_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_repeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mrmses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimputed_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimputed_prediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimputed_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8a27f5c7bae1>\u001b[0m in \u001b[0;36mmodified_autoencoder\u001b[0;34m(data, num_hidden, dropout, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     autoencoder.fit(data_imputed_and_scaled, data_scaled_with_nan, \n\u001b[0;32m--> 136\u001b[0;31m                     verbose=0, epochs=10000, callbacks=[early_stopping])\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"trained autoencoder\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2075\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from fancyimpute import SimpleFill, KNN, SoftImpute, IterativeSVD, MICE, MatrixFactorization, NuclearNormMinimization\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from keras.objectives import binary_crossentropy, mean_squared_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "# root mean squared error with three masks\n",
    "def rmse (original_data, y_pred, y_true): \n",
    "    # rsme prediction and ground truth\n",
    "    rmse_no_mask = np.sqrt(mse(y_true, y_pred))\n",
    "    \n",
    "    # ignore all zeros in the ground truth data\n",
    "    no_zeros = y_true > 0\n",
    "    rmse_no_zeros = np.sqrt(mse(y_true[no_zeros], y_pred[no_zeros]))\n",
    "    \n",
    "    # ignore zeros and only consider data that was originally nan in the training data\n",
    "    nan_no_zeros = np.isnan(original_data) & (y_true > 0)\n",
    "    rmse_nan_no_zeros = np.sqrt(mse(y_true[nan_no_zeros], y_pred[nan_no_zeros]))\n",
    "    \n",
    "    # concatenate all three results\n",
    "    return np.array([rmse_no_mask, rmse_no_zeros, rmse_nan_no_zeros])\n",
    "\n",
    "# compute mean rmse across a number of repreats\n",
    "def mean_rmse(data, imputation_method, y_true, num_repeats=1, **kwargs):\n",
    "    \n",
    "    imputed_predictions = [imputation_method(data, **kwargs) for i in range(num_repeats)]\n",
    "    \n",
    "    rmses = np.array([rmse(data, imputed_prediction, y_true) for imputed_prediction in imputed_predictions])\n",
    "\n",
    "    return rmses.mean(axis=0)\n",
    "\n",
    "# imputation methods\n",
    "# impute with sample mean\n",
    "def sample_mean(data, **kwargs):\n",
    "    fill = SimpleFill(fill_method=\"mean\")\n",
    "    return fill.complete(data)\n",
    "\n",
    "# impute with knn-3\n",
    "def knn_3(data, **kwargs):\n",
    "    fill = KNN(k=3, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# impute with knn-5\n",
    "def knn_5(data, **kwargs):\n",
    "    fill = KNN(k=5, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# knn for any k\n",
    "def knn(data, k, **kwargs):\n",
    "    fill = KNN(k=k, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# softimpute from fancyimpute package\n",
    "def soft_impute(data, **kwargs):\n",
    "    fill = SoftImpute(verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# removing to focus on optimising soft impute\n",
    "\n",
    "# # iterativeSVD from fancy impute package\n",
    "# def iterative_SVD(data, **kwargs):\n",
    "#     fill = IterativeSVD(verbose=0)\n",
    "#     return fill.complete(data)\n",
    "\n",
    "# # MICE for fancyimpute package\n",
    "# def mice(data, **kwargs):\n",
    "#     fill = MICE(verbose=0)\n",
    "#     return fill.complete(data)\n",
    "\n",
    "# modified autoencoder that does not propagate error from missing values\n",
    "def modified_autoencoder(data, num_hidden=[32], dropout=0.1, **kwargs):\n",
    "    \n",
    "    # dimensionality of data\n",
    "    num_proteins, num_features = data.shape\n",
    "    \n",
    "    # to normalise the data we must impute \n",
    "    mean_imputer = SimpleFill(fill_method=\"mean\")\n",
    "    data_imputed = mean_imputer.complete(data)\n",
    "    \n",
    "    # standard scaling for normalisation\n",
    "    standard_scaler = StandardScaler()\n",
    "    data_imputed_and_scaled = standard_scaler.fit_transform(data_imputed)\n",
    "    \n",
    "    # replace all missing values with 0 so they do not contribute to input\n",
    "    data_imputed_and_scaled[np.isnan(data)] = 0\n",
    "    \n",
    "    # maintain nan in target data so we know which outputs should not prodice any error\n",
    "    data_scaled_with_nan = np.array([[data_imputed_and_scaled[i, j] if ~np.isnan(data[i, j]) else np.nan\n",
    "                                     for j in range(num_features)] for i in range(num_proteins)])\n",
    "    \n",
    "    # custom MSE that only produces error on non-nan terms\n",
    "    def custom_MSE(y_true, y_pred):\n",
    "    \n",
    "        y_true = K.flatten(y_true)\n",
    "        y_pred = K.flatten(y_pred)\n",
    "\n",
    "        # mask for targets that are not nan\n",
    "        mask = ~tf.is_nan(y_true)\n",
    "\n",
    "        # apply the mask to targets and output of network and then compute MSE with what remains\n",
    "        y_true = tf.boolean_mask(tensor=y_true, mask=mask)\n",
    "        y_pred = tf.boolean_mask(tensor=y_pred, mask=mask)\n",
    "\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    # construct model\n",
    "    x = Input(shape=(num_features,))\n",
    "    \n",
    "    # first fully connected layer layer\n",
    "    y = Dense(num_hidden[0], activation=\"relu\")(x)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(dropout)(y)\n",
    "\n",
    "    # all remaining fully connected layers\n",
    "    for h in num_hidden[1:] + num_hidden[-2::-1]:\n",
    "        y = Dense(h, activation=\"relu\")(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(dropout)(y)\n",
    "    \n",
    "    # output -- no activation function \n",
    "    y = Dense(num_features, activation=\"linear\")(y)\n",
    "    autoencoder = Model(x, y)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=custom_MSE)\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience=100, min_delta=0)\n",
    "    # train model\n",
    "    autoencoder.fit(data_imputed_and_scaled, data_scaled_with_nan, \n",
    "                    verbose=0, epochs=10000, callbacks=[early_stopping])\n",
    "    \n",
    "    print \"trained autoencoder\"\n",
    "    # predict data\n",
    "    prediction = autoencoder.predict(data_imputed_and_scaled)\n",
    "    \n",
    "    # reverse normalise and return\n",
    "    return standard_scaler.inverse_transform(prediction)\n",
    "\n",
    "# PCA and then autoencoder\n",
    "def pca_autoencoder(data, num_hidden=[32], dropout=0.1, pca_dim=64, **kwargs):\n",
    "    \n",
    "    \n",
    "    # dimensionality of data\n",
    "    num_proteins, num_features = data.shape\n",
    "    \n",
    "    #construct model\n",
    "    x = Input(shape=(pca_dim,))\n",
    "    y = Dropout(1e-8)(x)\n",
    "    for h in num_hidden + num_hidden[-2::-1]:\n",
    "        y = Dense(h, activation=\"relu\")(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(dropout)(y)\n",
    "    y = Dense(pca_dim)(y)\n",
    "    \n",
    "    autoencoder = Model(x, y)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    \n",
    "    \n",
    "    # project with pca\n",
    "    mean_imputer = SimpleFill()\n",
    "    data_imputed = mean_imputer.complete(data)\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    data_transformed = pca.fit_transform(data_imputed)\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience=1000, min_delta=0)\n",
    "    autoencoder.fit(data_transformed, data_transformed, \n",
    "                    verbose=0, epochs=10000, callbacks=[early_stopping])\n",
    "    \n",
    "    prediction = autoencoder.predict(data_transformed)\n",
    "    \n",
    "    return pca.inverse_transform(prediction)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print \"Loading data\"\n",
    "\n",
    "    # training data\n",
    "    dfs = [pd.read_csv(\"../data/sub_challenge_1/data_obs_{}.txt\".format(i), \n",
    "                    header=0, index_col=0, sep=\"\\t\") for i in range(1, 11)]\n",
    "\n",
    "    # ground truth\n",
    "    ground_truth_table = pd.read_csv(\"../data/sub_challenge_1/data_true.txt\", \n",
    "                    header=0, index_col=0, sep=\"\\t\")\n",
    "\n",
    "    # conver from data frame ot numpy array\n",
    "    datas = [df.values for df in dfs]\n",
    "    ground_truth = ground_truth_table.values\n",
    "\n",
    "    # list of imputation tecniques\n",
    "#     imputation_methods = [sample_mean, knn_3, knn_5, soft_impute, \n",
    "#                           modified_autoencoder, pca_autoencoder]\n",
    "#     imputation_method_names = [\"sample_mean\", \"knn_3\", \"knn_5\", \"soft_impute\", \n",
    "#                           \"modified_autoencoder\", \"pca_autoencoder\"]\n",
    "\n",
    "    imputation_methods = [partial(modified_autoencoder, num_hidden=[32]),#]\n",
    "                         partial(modified_autoencoder, num_hidden=[64, 32]),\n",
    "                         partial(modified_autoencoder, num_hidden=[128, 64, 32])]\n",
    "    imputation_method_names = [\"autoencoder_32\", #]\n",
    "                               \"autoencoder_64_32\", \n",
    "                               \"autoencoder_128_64_32\"]\n",
    "    \n",
    "    print \"Computing rmse\"\n",
    "    \n",
    "    # iterate over all training data and imputation methods and compute mean rmse for num repeats\n",
    "    rmses = np.array([[mean_rmse(data, imputation_method, ground_truth, num_repeats=1) for data in datas] \n",
    "                      for imputation_method in imputation_methods])\n",
    "    \n",
    "    print \"Saving rmse to file\"\n",
    "    \n",
    "    # save to file\n",
    "    np.savetxt(X=rmses[:,:,0], \n",
    "               fname=\"../results/subchallenge_1/{}_rmses_no_mask.csv\".format(\"_\".join(imputation_method_names)), delimiter=\",\")\n",
    "    np.savetxt(X=rmses[:,:,1], \n",
    "               fname=\"../results/subchallenge_1/{}_rmses_ignore_zeros.csv\".format(\"_\".join(imputation_method_names)), delimiter=\",\")\n",
    "    np.savetxt(X=rmses[:,:,2], \n",
    "               fname=\"../results/subchallenge_1/{}_rmses_only_nan_ignore_zeros.csv\".format(\"_\".join(imputation_method_names)), \n",
    "               delimiter=\",\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
