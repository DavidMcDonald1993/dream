{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Computing rmse\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-42205bfebcd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-42205bfebcd0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# iterate over all training data and imputation methods and compute mean rmse for num repeats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     rmses = np.array([[mean_rmse(data, imputation_method, ground_truth, num_repeats=1) for data in datas] \n\u001b[0;32m--> 196\u001b[0;31m                       for imputation_method in imputation_methods])\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Saving rmse to file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-42205bfebcd0>\u001b[0m in \u001b[0;36mmean_rmse\u001b[0;34m(data, imputation_method, y_true, num_repeats, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_rmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimputation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mimputed_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimputation_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_repeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mrmses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimputed_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimputed_prediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimputed_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-42205bfebcd0>\u001b[0m in \u001b[0;36mmice\u001b[0;34m(data, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mfill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMICE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# modified autoencoder that does not propagate error from missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/fancyimpute/mice.pyc\u001b[0m in \u001b[0;36mcomplete\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[MICE] Completing matrix with shape %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mX_completed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mimputed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_imputations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0;31m# average the imputed values for each feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0maverage_imputated_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputed_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/fancyimpute/mice.pyc\u001b[0m in \u001b[0;36mmultiple_imputations\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mmissing_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mobserved_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 visit_indices=visit_indices)\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_burn_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mresults_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_filled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda2/lib/python2.7/site-packages/fancyimpute/mice.pyc\u001b[0m in \u001b[0;36mperform_imputation_round\u001b[0;34m(self, X_filled, missing_mask, observed_mask, visit_indices)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         p=p)\n\u001b[1;32m    196\u001b[0m                 \u001b[0mX_other_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_filled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_column_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mX_other_cols_observed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_other_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobserved_row_mask_for_this_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m                 \u001b[0mbrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 brr.fit(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from fancyimpute import SimpleFill, KNN, SoftImpute, IterativeSVD, MICE, MatrixFactorization, NuclearNormMinimization\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from keras.objectives import binary_crossentropy, mean_squared_error\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "# root mean squared error with three masks\n",
    "def rmse (original_data, y_pred, y_true): \n",
    "    # rsme prediction and ground truth\n",
    "    rmse_no_mask = np.sqrt(mse(y_true, y_pred))\n",
    "    \n",
    "    # ignore all zeros in the ground truth data\n",
    "    no_zeros = y_true > 0\n",
    "    rmse_no_zeros = np.sqrt(mse(y_true[no_zeros], y_pred[no_zeros]))\n",
    "    \n",
    "    # ignore zeros and only consider data that was originally nan in the training data\n",
    "    nan_no_zeros = np.isnan(original_data) & (y_true > 0)\n",
    "    rmse_nan_no_zeros = np.sqrt(mse(y_true[nan_no_zeros], y_pred[nan_no_zeros]))\n",
    "    \n",
    "    # concatenate all three results\n",
    "    return np.array([rmse_no_mask, rmse_no_zeros, rmse_nan_no_zeros])\n",
    "\n",
    "# compute mean rmse across a number of repreats\n",
    "def mean_rmse(data, imputation_method, y_true, num_repeats=1, **kwargs):\n",
    "    \n",
    "    imputed_predictions = [imputation_method(data, **kwargs) for i in range(num_repeats)]\n",
    "    \n",
    "    rmses = np.array([rmse(data, imputed_prediction, y_true) for imputed_prediction in imputed_predictions])\n",
    "\n",
    "    return rmses.mean(axis=0)\n",
    "\n",
    "# imputation methods\n",
    "# impute with sample mean\n",
    "def sample_mean(data, **kwargs):\n",
    "    fill = SimpleFill(fill_method=\"mean\")\n",
    "    return fill.complete(data)\n",
    "\n",
    "# impute with knn-3\n",
    "def knn_3(data, **kwargs):\n",
    "    fill = KNN(k=3, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# impute with knn-5\n",
    "def knn_5(data, **kwargs):\n",
    "    fill = KNN(k=5, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# knn for any k\n",
    "def knn(data, k, **kwargs):\n",
    "    fill = KNN(k=k, verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# softimpute from fancyimpute package\n",
    "def soft_impute(data, **kwargs):\n",
    "    fill = SoftImpute(verbose=0)\n",
    "    return fill.complete(data)\n",
    "\n",
    "# removing to focus on optimising soft impute\n",
    "\n",
    "# # iterativeSVD from fancy impute package\n",
    "# def iterative_SVD(data, **kwargs):\n",
    "#     fill = IterativeSVD(verbose=0)\n",
    "#     return fill.complete(data)\n",
    "\n",
    "# # MICE for fancyimpute package\n",
    "# def mice(data, **kwargs):\n",
    "#     fill = MICE(verbose=0)\n",
    "#     return fill.complete(data)\n",
    "\n",
    "# modified autoencoder that does not propagate error from missing values\n",
    "def modified_autoencoder(data, num_hidden=[32], dropout=0.1, **kwargs):\n",
    "    \n",
    "    # to normalise the data we must impute \n",
    "    mean_imputer = SimpleFill(fill_method=\"mean\")\n",
    "    data_imputed = mean_imputer.complete(data)\n",
    "    \n",
    "    # standard scaling for normalisation\n",
    "    standard_scaler = StandardScaler()\n",
    "    data_imputed_and_scaled = standard_scaler.fit_transform(data_imputed)\n",
    "    \n",
    "    # replace all missing values with 0 so they do not contribute to input\n",
    "    data_imputed_and_scaled[np.isnan(data)] = 0\n",
    "    \n",
    "    # maintain nan in target data so we know which outputs should not prodice any error\n",
    "    data_scaled_with_nan = np.array([[data_imputed_and_scaled[i, j] if ~np.isnan(data[i, j]) else np.nan\n",
    "                                     for j in range(num_features)] for i in range(num_samples)])\n",
    "    \n",
    "    # custom MSE that only produces error on non-nan terms\n",
    "    def custom_MSE(y_true, y_pred):\n",
    "    \n",
    "        y_true = K.flatten(y_true)\n",
    "        y_pred = K.flatten(y_pred)\n",
    "\n",
    "        # mask for targets that are not nan\n",
    "        mask = ~tf.is_nan(y_true)\n",
    "\n",
    "        # apply the mask to targets and output of network and then compute MSE with what remains\n",
    "        y_true = tf.boolean_mask(tensor=y_true, mask=mask)\n",
    "        y_pred = tf.boolean_mask(tensor=y_pred, mask=mask)\n",
    "\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    # construct model\n",
    "    x = Input(shape=(num_features,))\n",
    "    \n",
    "    # first fully connected layer layer\n",
    "    y = Dense(num_hidden[0], activation=\"relu\")(x)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(dropout)(y)\n",
    "\n",
    "    # all remaining fully connected layers\n",
    "    for h in num_hidden[1:] + num_hidden[-2::-1]:\n",
    "        y = Dense(h, activation=\"relu\")(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(dropout)(y)\n",
    "    \n",
    "    # output -- no activation function \n",
    "    y = Dense(num_features, activation=\"linear\")(y)\n",
    "    autoencoder = Model(x, y)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=custom_binary_crossentropy)\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience=1000, min_delta=0)\n",
    "    # train model\n",
    "    autoencoder.fit(data_imputed_and_scaled, data_scaled_with_nan, \n",
    "                    verbose=0, epochs=10000, batch_size=100, callbacks=[early_stopping])\n",
    "    # predict data\n",
    "    prediction = autoencoder.predict(data_imputed_and_scaled)\n",
    "    \n",
    "    # reverse normalise and return\n",
    "    return standard_scaler.inverse_transform(prediction)\n",
    "\n",
    "# PCA and then autoencoder\n",
    "def pca_autoencoder(data, num_hidden=[32], dropout=0.1, pca_dim=64, **kwargs):\n",
    "    \n",
    "    #construct model\n",
    "    x = Input(shape=(pca_dim,))\n",
    "    y = Dropout(1e-8)(x)\n",
    "    for h in num_hidden + num_hidden[-2::-1]:\n",
    "        y = Dense(h, activation=\"relu\")(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(dropout)(y)\n",
    "    y = Dense(pca_dim)(y)\n",
    "    \n",
    "    autoencoder = Model(x, y)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    \n",
    "    \n",
    "    # project with pca\n",
    "    mean_imputer = SimpleFill()\n",
    "    data_imputed = mean_imputer.complete(data)\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    data_transformed = pca.fit_transform(data_imputed)\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience=100, min_delta=0)\n",
    "    autoencoder.fit(data_transformed, data_transformed, \n",
    "                    verbose=0, epochs=10000, batch_size=100, callbacks=[early_stopping])\n",
    "    \n",
    "    prediction = autoencoder.predict(data_transformed)\n",
    "    \n",
    "    return pca.inverse_transform(prediction)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print \"Loading data\"\n",
    "\n",
    "    # training data\n",
    "    dfs = [pd.read_csv(\"../data/sub_challenge_1/data_obs_{}.txt\".format(i), \n",
    "                    header=0, index_col=0, sep=\"\\t\") for i in range(1, 11)]\n",
    "\n",
    "    # ground truth\n",
    "    ground_truth_table = pd.read_csv(\"../data/sub_challenge_1/data_true.txt\", \n",
    "                    header=0, index_col=0, sep=\"\\t\")\n",
    "\n",
    "    # conver from data frame ot numpy array\n",
    "    datas = [df.values for df in dfs]\n",
    "    ground_truth = ground_truth_table.values\n",
    "\n",
    "    # dimensionality of data\n",
    "    num_proteins, num_features = ground_truth.shape\n",
    "\n",
    "    # list of imputation tecniques\n",
    "    imputation_methods = [sample_mean, knn_3, knn_5, soft_impute, \n",
    "                          modified_autoencoder, pca_autoencoder]\n",
    "#     imputation_methods = [sample_mean]\n",
    "    \n",
    "    print \"Computing rmse\"\n",
    "    \n",
    "    # iterate over all training data and imputation methods and compute mean rmse for num repeats\n",
    "    rmses = np.array([[mean_rmse(data, imputation_method, ground_truth, num_repeats=1) for data in datas] \n",
    "                      for imputation_method in imputation_methods])\n",
    "    \n",
    "    print \"Saving rmse to file\"\n",
    "    \n",
    "    # save to file\n",
    "    np.savetxt(X=rmses[:,:,0], \n",
    "               fname=\"../results/subchallenge_1/{}_rmses_no_mask.csv\".format(imputation_methods), delimiter=\",\")\n",
    "    np.savetxt(X=rmses[:,:,1], \n",
    "               fname=\"../results/subchallenge_1/{}_rmses_ignore_zeros.csv\".format(imputation_methods), delimiter=\",\")\n",
    "    np.savetxt(X=rmses[:,:,2], \n",
    "               fname=\"../results/subchallenge_1/{}_rmses_only_nan_ignore_zeros.csv\".format(imputation_methods), \n",
    "               delimiter=\",\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
